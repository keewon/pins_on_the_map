#!/usr/bin/env python3
"""ì§€í•˜ì² /êµ­ì² ì—­ ìœ„ì¹˜ ìˆ˜ì§‘"""

import json
import os
import re
import requests
from dotenv import load_dotenv

load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

NAME = "ì§€í•˜ì² ì—­"
LIST_ID = 6
API_KEY = os.environ.get('KAKAO_API_KEY')

# ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ (ì§€ì—­ + í˜¸ì„ ë³„ë¡œ ì„¸ë¶„í™”)
SEARCH_QUERIES = [
    # ìˆ˜ë„ê¶Œ í˜¸ì„ ë³„
    "ì„œìš¸ 1í˜¸ì„ ", "ì„œìš¸ 2í˜¸ì„ ", "ì„œìš¸ 3í˜¸ì„ ", "ì„œìš¸ 4í˜¸ì„ ",
    "ì„œìš¸ 5í˜¸ì„ ", "ì„œìš¸ 6í˜¸ì„ ", "ì„œìš¸ 7í˜¸ì„ ", "ì„œìš¸ 8í˜¸ì„ ", "ì„œìš¸ 9í˜¸ì„ ",
    "ê²½ì˜ì¤‘ì•™ì„ ", "ë¶„ë‹¹ì„ ", "ì‹ ë¶„ë‹¹ì„ ", "ê²½ì¶˜ì„ ", "ê²½ê°•ì„ ", "ì„œí•´ì„ ",
    "ìˆ˜ì¸ë¶„ë‹¹ì„ ", "ê³µí•­ì² ë„", "ì‹ ë¦¼ì„ ", "ìš°ì´ì‹ ì„¤ì„ ",
    "ê¹€í¬ê³¨ë“œë¼ì¸", "ì—ë²„ë¼ì¸", "ì˜ì •ë¶€ê²½ì „ì² ",
    # ì¸ì²œ
    "ì¸ì²œ 1í˜¸ì„ ", "ì¸ì²œ 2í˜¸ì„ ", "ì¸ì²œì§€í•˜ì² ",
    # ë¶€ì‚°
    "ë¶€ì‚° 1í˜¸ì„ ", "ë¶€ì‚° 2í˜¸ì„ ", "ë¶€ì‚° 3í˜¸ì„ ", "ë¶€ì‚° 4í˜¸ì„ ", "ë¶€ì‚°ì§€í•˜ì² ",
    "ë™í•´ì„  ì „ì² ", "ë¶€ì‚°ê¹€í•´ê²½ì „ì² ",
    # ëŒ€êµ¬
    "ëŒ€êµ¬ 1í˜¸ì„ ", "ëŒ€êµ¬ 2í˜¸ì„ ", "ëŒ€êµ¬ 3í˜¸ì„ ", "ëŒ€êµ¬ì§€í•˜ì² ",
    # ëŒ€ì „/ê´‘ì£¼
    "ëŒ€ì „ 1í˜¸ì„ ", "ëŒ€ì „ì§€í•˜ì² ", "ê´‘ì£¼ 1í˜¸ì„ ", "ê´‘ì£¼ì§€í•˜ì² ",
    # ì¼ë°˜ ê²€ìƒ‰
    "ì§€í•˜ì² ì—­", "ì „ì² ì—­",
]


def fetch_stations():
    """ì§€í•˜ì² ì—­ ê²€ìƒ‰"""
    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'
    headers = {'Authorization': f'KakaoAK {API_KEY}'}
    
    all_results = []
    seen_ids = set()
    
    for query in SEARCH_QUERIES:
        page = 1
        
        while page <= 45:  # max 45 pages
            params = {'query': query, 'size': 15, 'page': page}
            response = requests.get(url, headers=headers, params=params)
            data = response.json()
            
            for doc in data.get('documents', []):
                if doc['id'] not in seen_ids:
                    seen_ids.add(doc['id'])
                    all_results.append(doc)
            
            if data.get('meta', {}).get('is_end', True):
                break
            page += 1
        
        print(f"  {query}: {len(seen_ids)}ê°œ ëˆ„ì ")
    
    return all_results


def extract_lines(name, category):
    """ì—­ ì´ë¦„ì´ë‚˜ ì¹´í…Œê³ ë¦¬ì—ì„œ í˜¸ì„  ì •ë³´ ì¶”ì¶œ"""
    lines = []
    
    # ì¹´í…Œê³ ë¦¬ì—ì„œ ì¶”ì¶œ (ì˜ˆ: "êµí†µ,ìˆ˜ì†¡ > ì§€í•˜ì² ,ì „ì²  > ìˆ˜ë„ê¶Œ1í˜¸ì„ ")
    if 'í˜¸ì„ ' in category:
        match = re.search(r'(\d+í˜¸ì„ )', category)
        if match:
            lines.append(match.group(1))
    
    # ë…¸ì„  ì´ë¦„ ë§¤í•‘
    line_keywords = {
        '1í˜¸ì„ ': '1í˜¸ì„ ', '2í˜¸ì„ ': '2í˜¸ì„ ', '3í˜¸ì„ ': '3í˜¸ì„ ', '4í˜¸ì„ ': '4í˜¸ì„ ',
        '5í˜¸ì„ ': '5í˜¸ì„ ', '6í˜¸ì„ ': '6í˜¸ì„ ', '7í˜¸ì„ ': '7í˜¸ì„ ', '8í˜¸ì„ ': '8í˜¸ì„ ',
        '9í˜¸ì„ ': '9í˜¸ì„ ',
        'ê²½ì˜ì„ ': 'ê²½ì˜ì¤‘ì•™ì„ ', 'ì¤‘ì•™ì„ ': 'ê²½ì˜ì¤‘ì•™ì„ ', 'ê²½ì˜ì¤‘ì•™': 'ê²½ì˜ì¤‘ì•™ì„ ',
        'ê²½ì¶˜ì„ ': 'ê²½ì¶˜ì„ ', 'ë¶„ë‹¹ì„ ': 'ë¶„ë‹¹ì„ ', 'ì‹ ë¶„ë‹¹ì„ ': 'ì‹ ë¶„ë‹¹ì„ ',
        'ê²½ê°•ì„ ': 'ê²½ê°•ì„ ', 'ìˆ˜ì¸ì„ ': 'ìˆ˜ì¸ë¶„ë‹¹ì„ ', 'ìˆ˜ì¸ë¶„ë‹¹': 'ìˆ˜ì¸ë¶„ë‹¹ì„ ',
        'ê³µí•­ì² ë„': 'ê³µí•­ì² ë„', 'ì¸ì²œ1í˜¸ì„ ': 'ì¸ì²œ1í˜¸ì„ ', 'ì¸ì²œ2í˜¸ì„ ': 'ì¸ì²œ2í˜¸ì„ ',
        'GTX': 'GTX-A', 'ì‹ ë¦¼ì„ ': 'ì‹ ë¦¼ì„ ', 'ìš°ì´ì‹ ì„¤': 'ìš°ì´ì‹ ì„¤ì„ ',
        'ê¹€í¬ê³¨ë“œ': 'ê¹€í¬ê³¨ë“œë¼ì¸', 'ì—ë²„ë¼ì¸': 'ì—ë²„ë¼ì¸', 'ìš©ì¸ê²½ì „ì² ': 'ì—ë²„ë¼ì¸',
        'ì˜ì •ë¶€': 'ì˜ì •ë¶€ê²½ì „ì² ', 'ì„œí•´ì„ ': 'ì„œí•´ì„ ',
        'ë¶€ì‚°1í˜¸ì„ ': 'ë¶€ì‚°1í˜¸ì„ ', 'ë¶€ì‚°2í˜¸ì„ ': 'ë¶€ì‚°2í˜¸ì„ ', 'ë¶€ì‚°3í˜¸ì„ ': 'ë¶€ì‚°3í˜¸ì„ ', 'ë¶€ì‚°4í˜¸ì„ ': 'ë¶€ì‚°4í˜¸ì„ ',
        'ëŒ€êµ¬1í˜¸ì„ ': 'ëŒ€êµ¬1í˜¸ì„ ', 'ëŒ€êµ¬2í˜¸ì„ ': 'ëŒ€êµ¬2í˜¸ì„ ', 'ëŒ€êµ¬3í˜¸ì„ ': 'ëŒ€êµ¬3í˜¸ì„ ',
        'ëŒ€ì „1í˜¸ì„ ': 'ëŒ€ì „1í˜¸ì„ ', 'ê´‘ì£¼1í˜¸ì„ ': 'ê´‘ì£¼1í˜¸ì„ ',
        'KTX': 'KTX', 'SRT': 'SRT', 'ìƒˆë§ˆì„': 'ì¼ë°˜ì² ë„', 'ë¬´ê¶í™”': 'ì¼ë°˜ì² ë„',
    }
    
    text = name + ' ' + category
    for keyword, line_name in line_keywords.items():
        if keyword in text and line_name not in lines:
            lines.append(line_name)
    
    return lines


def filter_station(doc):
    """ì§€í•˜ì² /ì „ì² ì—­ë§Œ í•„í„°ë§"""
    name = doc.get('place_name', '')
    category = doc.get('category_name', '')
    
    # ì§€í•˜ì² /ì „ì²  ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš°
    if 'ì§€í•˜ì² ' in category or 'ì „ì² ' in category:
        # ì œì™¸ í‚¤ì›Œë“œ
        exclude = ['ì£¼ì°¨ì¥', 'í™”ì¥ì‹¤', 'í¸ì˜ì ', 'ë³´ê´€í•¨', 'ì¶œì…êµ¬', 'í™˜ìŠ¹', 'ëŒ€í•©ì‹¤', 'ë§¤í‘œì†Œ']
        for ex in exclude:
            if ex in name:
                return False
        return True
    
    # ì—­ ì´ë¦„ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
    if name.endswith('ì—­') and ('êµí†µ' in category or 'ì² ë„' in category):
        return True
    
    return False


def get_region(doc):
    """ì£¼ì†Œì—ì„œ ê´‘ì—­ë‹¨ì²´ ì¶”ì¶œ"""
    addr = doc.get('road_address_name') or doc.get('address_name') or ''
    parts = addr.split()
    return parts[0] if parts else 'ê¸°íƒ€'


def convert_to_pins(docs):
    """ì¹´ì¹´ì˜¤ API ê²°ê³¼ë¥¼ í•€ ë°ì´í„°ë¡œ ë³€í™˜"""
    pins = []
    
    for doc in docs:
        name = doc.get('place_name', '')
        category = doc.get('category_name', '')
        lines = extract_lines(name, category)
        
        description = ', '.join(lines) if lines else ''
        
        pins.append({
            "title": name,
            "lat": float(doc.get('y')),
            "lng": float(doc.get('x')),
            "address": doc.get('road_address_name') or doc.get('address_name'),
            "description": description,
            "url": doc.get('place_url'),
            "region": get_region(doc)
        })
    
    return pins


def main():
    print(f"ğŸš‡ {NAME} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # 1. Fetch
    raw = fetch_stations()
    print(f"ğŸ“¥ ê²€ìƒ‰ ê²°ê³¼: {len(raw)}ê°œ")
    
    # 2. Save raw
    raw_path = os.path.join(os.path.dirname(__file__), f'{NAME}_raw.json')
    with open(raw_path, 'w', encoding='utf-8') as f:
        json.dump(raw, f, ensure_ascii=False, indent=2)
    
    # 3. Filter
    filtered = [doc for doc in raw if filter_station(doc)]
    print(f"ğŸ” í•„í„°ë§ í›„: {len(filtered)}ê°œ")
    
    # 4. Convert to pins
    pins = convert_to_pins(filtered)
    
    # 5. Remove duplicates by title + address
    seen = set()
    unique_pins = []
    for pin in pins:
        key = (pin['title'], pin['address'])
        if key not in seen:
            seen.add(key)
            unique_pins.append(pin)
    
    print(f"âœ¨ ì¤‘ë³µ ì œê±° í›„: {len(unique_pins)}ê°œ")
    
    # 6. Save
    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', f'{LIST_ID}.json')
    with open(data_path, 'w', encoding='utf-8') as f:
        json.dump({"pins": unique_pins}, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {data_path} ì €ì¥ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

