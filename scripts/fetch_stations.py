#!/usr/bin/env python3
"""ì§€í•˜ì² /êµ­ì² ì—­ ìœ„ì¹˜ ìˆ˜ì§‘"""

import json
import os
import re
import math
import requests
from dotenv import load_dotenv

load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

NAME = "ì§€í•˜ì² ì—­"
LIST_ID = 6
API_KEY = os.environ.get('KAKAO_API_KEY')

# ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ (ì§€ì—­ + í˜¸ì„ ë³„ë¡œ ì„¸ë¶„í™”)
SEARCH_QUERIES = [
    # ìˆ˜ë„ê¶Œ í˜¸ì„ ë³„
    "ì„œìš¸ 1í˜¸ì„ ", "ì„œìš¸ 2í˜¸ì„ ", "ì„œìš¸ 3í˜¸ì„ ", "ì„œìš¸ 4í˜¸ì„ ",
    "ì„œìš¸ 5í˜¸ì„ ", "ì„œìš¸ 6í˜¸ì„ ", "ì„œìš¸ 7í˜¸ì„ ", "ì„œìš¸ 8í˜¸ì„ ", "ì„œìš¸ 9í˜¸ì„ ",
    "ê²½ì˜ì¤‘ì•™ì„ ", "ë¶„ë‹¹ì„ ", "ì‹ ë¶„ë‹¹ì„ ", "ê²½ì¶˜ì„ ", "ê²½ê°•ì„ ", "ì„œí•´ì„ ",
    "ìˆ˜ì¸ë¶„ë‹¹ì„ ", "ê³µí•­ì² ë„", "ì‹ ë¦¼ì„ ", "ìš°ì´ì‹ ì„¤ì„ ",
    "ê¹€í¬ê³¨ë“œë¼ì¸", "ì—ë²„ë¼ì¸", "ì˜ì •ë¶€ê²½ì „ì² ",
    # ì¸ì²œ
    "ì¸ì²œ 1í˜¸ì„ ", "ì¸ì²œ 2í˜¸ì„ ", "ì¸ì²œì§€í•˜ì² ",
    # ë¶€ì‚°
    "ë¶€ì‚° 1í˜¸ì„ ", "ë¶€ì‚° 2í˜¸ì„ ", "ë¶€ì‚° 3í˜¸ì„ ", "ë¶€ì‚° 4í˜¸ì„ ", "ë¶€ì‚°ì§€í•˜ì² ",
    "ë™í•´ì„  ì „ì² ", "ë¶€ì‚°ê¹€í•´ê²½ì „ì² ",
    # ëŒ€êµ¬
    "ëŒ€êµ¬ 1í˜¸ì„ ", "ëŒ€êµ¬ 2í˜¸ì„ ", "ëŒ€êµ¬ 3í˜¸ì„ ", "ëŒ€êµ¬ì§€í•˜ì² ",
    # ëŒ€ì „/ê´‘ì£¼
    "ëŒ€ì „ 1í˜¸ì„ ", "ëŒ€ì „ì§€í•˜ì² ", "ê´‘ì£¼ 1í˜¸ì„ ", "ê´‘ì£¼ì§€í•˜ì² ",
    # ì¼ë°˜ ê²€ìƒ‰
    "ì§€í•˜ì² ì—­", "ì „ì² ì—­",
]


def fetch_stations():
    """ì§€í•˜ì² ì—­ ê²€ìƒ‰"""
    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'
    headers = {'Authorization': f'KakaoAK {API_KEY}'}
    
    all_results = []
    seen_ids = set()
    
    for query in SEARCH_QUERIES:
        page = 1
        
        while page <= 45:  # max 45 pages
            params = {'query': query, 'size': 15, 'page': page}
            response = requests.get(url, headers=headers, params=params)
            data = response.json()
            
            for doc in data.get('documents', []):
                if doc['id'] not in seen_ids:
                    seen_ids.add(doc['id'])
                    all_results.append(doc)
            
            if data.get('meta', {}).get('is_end', True):
                break
            page += 1
        
        print(f"  {query}: {len(seen_ids)}ê°œ ëˆ„ì ")
    
    return all_results


def extract_lines(name, category):
    """ì—­ ì´ë¦„ì´ë‚˜ ì¹´í…Œê³ ë¦¬ì—ì„œ í˜¸ì„  ì •ë³´ ì¶”ì¶œ"""
    lines = []
    text = name + ' ' + category
    
    # ì§€ì—­ í˜¸ì„  ë¨¼ì € í™•ì¸ (ë¶€ì‚°4í˜¸ì„ , ëŒ€êµ¬3í˜¸ì„  ë“±)
    regional_patterns = [
        (r'ë¶€ì‚°(\d)í˜¸ì„ ', 'ë¶€ì‚°{}í˜¸ì„ '),
        (r'ëŒ€êµ¬(\d)í˜¸ì„ ', 'ëŒ€êµ¬{}í˜¸ì„ '),
        (r'ëŒ€ì „(\d)í˜¸ì„ ', 'ëŒ€ì „{}í˜¸ì„ '),
        (r'ê´‘ì£¼(\d)í˜¸ì„ ', 'ê´‘ì£¼{}í˜¸ì„ '),
        (r'ì¸ì²œ(\d)í˜¸ì„ ', 'ì¸ì²œ{}í˜¸ì„ '),
    ]
    
    regional_line_nums = set()  # ì§€ì—­ í˜¸ì„ ì—ì„œ ì‚¬ìš©ëœ ë²ˆí˜¸ ê¸°ë¡
    
    for pattern, format_str in regional_patterns:
        matches = re.findall(pattern, text)
        for num in matches:
            line_name = format_str.format(num)
            if line_name not in lines:
                lines.append(line_name)
                regional_line_nums.add(num)
    
    # ìˆ˜ë„ê¶Œ í˜¸ì„  (ì§€ì—­ í˜¸ì„ ì—ì„œ ì‚¬ìš©ëœ ë²ˆí˜¸ëŠ” ì œì™¸)
    for num in ['1', '2', '3', '4', '5', '6', '7', '8', '9']:
        if f'{num}í˜¸ì„ ' in text and num not in regional_line_nums:
            line_name = f'{num}í˜¸ì„ '
            if line_name not in lines:
                lines.append(line_name)
    
    # ê¸°íƒ€ ë…¸ì„ 
    other_keywords = {
        'ê²½ì˜ì„ ': 'ê²½ì˜ì¤‘ì•™ì„ ', 'ì¤‘ì•™ì„ ': 'ê²½ì˜ì¤‘ì•™ì„ ', 'ê²½ì˜ì¤‘ì•™': 'ê²½ì˜ì¤‘ì•™ì„ ',
        'ê²½ì¶˜ì„ ': 'ê²½ì¶˜ì„ ', 'ë¶„ë‹¹ì„ ': 'ë¶„ë‹¹ì„ ', 'ì‹ ë¶„ë‹¹ì„ ': 'ì‹ ë¶„ë‹¹ì„ ',
        'ê²½ê°•ì„ ': 'ê²½ê°•ì„ ', 'ìˆ˜ì¸ì„ ': 'ìˆ˜ì¸ë¶„ë‹¹ì„ ', 'ìˆ˜ì¸ë¶„ë‹¹': 'ìˆ˜ì¸ë¶„ë‹¹ì„ ',
        'ê³µí•­ì² ë„': 'ê³µí•­ì² ë„', 'GTX': 'GTX-A', 'ì‹ ë¦¼ì„ ': 'ì‹ ë¦¼ì„ ', 'ìš°ì´ì‹ ì„¤': 'ìš°ì´ì‹ ì„¤ì„ ',
        'ê¹€í¬ê³¨ë“œ': 'ê¹€í¬ê³¨ë“œë¼ì¸', 'ì—ë²„ë¼ì¸': 'ì—ë²„ë¼ì¸', 'ìš©ì¸ê²½ì „ì² ': 'ì—ë²„ë¼ì¸',
        'ì˜ì •ë¶€': 'ì˜ì •ë¶€ê²½ì „ì² ', 'ì„œí•´ì„ ': 'ì„œí•´ì„ ', 'ë™í•´ì„ ': 'ë™í•´ì„ ',
        'ë¶€ì‚°ê¹€í•´ê²½ì „ì² ': 'ë¶€ì‚°ê¹€í•´ê²½ì „ì² ', 'ë¶€ì‚°ê¹€í•´': 'ë¶€ì‚°ê¹€í•´ê²½ì „ì² ',
    }
    
    for keyword, line_name in other_keywords.items():
        if keyword in text and line_name not in lines:
            lines.append(line_name)
    
    return lines


def filter_station(doc):
    """ì§€í•˜ì² /ì „ì² ì—­ë§Œ í•„í„°ë§"""
    name = doc.get('place_name', '')
    category = doc.get('category_name', '')
    
    # ì§€í•˜ì² /ì „ì²  ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš°
    if 'ì§€í•˜ì² ' in category or 'ì „ì² ' in category:
        # ì œì™¸ í‚¤ì›Œë“œ
        exclude = ['ì£¼ì°¨ì¥', 'í™”ì¥ì‹¤', 'í¸ì˜ì ', 'ë³´ê´€í•¨', 'ì¶œì…êµ¬', 'í™˜ìŠ¹', 'ëŒ€í•©ì‹¤', 'ë§¤í‘œì†Œ', 'ì¶œêµ¬']
        for ex in exclude:
            if ex in name:
                return False
        return True
    
    # ì—­ ì´ë¦„ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
    if name.endswith('ì—­') and ('êµí†µ' in category or 'ì² ë„' in category):
        return True
    
    return False


def get_region(doc):
    """ì£¼ì†Œì—ì„œ ê´‘ì—­ë‹¨ì²´ ì¶”ì¶œ"""
    addr = doc.get('road_address_name') or doc.get('address_name') or ''
    parts = addr.split()
    return parts[0] if parts else 'ê¸°íƒ€'


def haversine_distance(lat1, lng1, lat2, lng2):
    """ë‘ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (km)"""
    R = 6371  # ì§€êµ¬ ë°˜ê²½ (km)
    
    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lng = math.radians(lng2 - lng1)
    
    a = math.sin(delta_lat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lng/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    
    return R * c


def normalize_station_name(name):
    """ì—­ ì´ë¦„ì—ì„œ í˜¸ì„  ì •ë³´ ì œê±°í•˜ì—¬ ì •ê·œí™”"""
    # "ê³ ì´Œì—­ ê¹€í¬ê³¨ë“œë¼ì¸" -> "ê³ ì´Œì—­"
    # "ì„œìš¸ì—­ 1í˜¸ì„ " -> "ì„œìš¸ì—­"
    patterns = [
        r'\s+(ê¹€í¬ê³¨ë“œë¼ì¸|ì—ë²„ë¼ì¸|ì˜ì •ë¶€ê²½ì „ì² |ì‹ ë¶„ë‹¹ì„ |ê²½ì˜ì¤‘ì•™ì„ |ê³µí•­ì² ë„|ì‹ ë¦¼ì„ |ìš°ì´ì‹ ì„¤ì„ |ì„œí•´ì„ |ê²½ì¶˜ì„ |ê²½ê°•ì„ |ë¶„ë‹¹ì„ |ìˆ˜ì¸ë¶„ë‹¹ì„ )',
        r'\s+(ë¶€ì‚°\dí˜¸ì„ |ëŒ€êµ¬\dí˜¸ì„ |ëŒ€ì „\dí˜¸ì„ |ê´‘ì£¼\dí˜¸ì„ |ì¸ì²œ\dí˜¸ì„ )',
        r'\s+\dí˜¸ì„ ',
        r'\s+GTX-?[A-Z]?',
        r'\s+ë™í•´ì„ ',
        r'\s+ë¶€ì‚°ê¹€í•´ê²½ì „ì² ',
    ]
    
    result = name
    for pattern in patterns:
        result = re.sub(pattern, '', result)
    
    return result.strip()


def merge_transfer_stations(pins):
    """ê°€ê¹Œìš´ ê±°ë¦¬ì— ìˆëŠ” ê°™ì€ ì´ë¦„ì˜ ì—­ì„ í™˜ìŠ¹ì—­ìœ¼ë¡œ ë³‘í•©"""
    MERGE_DISTANCE_KM = 0.5  # 500m ì´ë‚´ë§Œ ë³‘í•©
    
    # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ê·¸ë£¹í™”
    name_groups = {}
    for pin in pins:
        normalized = normalize_station_name(pin['title'])
        if normalized not in name_groups:
            name_groups[normalized] = []
        name_groups[normalized].append(pin)
    
    merged_pins = []
    
    for name, group in name_groups.items():
        if len(group) == 1:
            # ë‹¨ì¼ ì—­
            pin = group[0]
            pin['title'] = name  # ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©
            merged_pins.append(pin)
        else:
            # ì—¬ëŸ¬ ê°œì˜ ê°™ì€ ì´ë¦„ ì—­ - ê±°ë¦¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
            clusters = []
            
            for pin in group:
                added_to_cluster = False
                
                for cluster in clusters:
                    # í´ëŸ¬ìŠ¤í„°ì˜ ì²« ë²ˆì§¸ ì—­ê³¼ ê±°ë¦¬ ë¹„êµ
                    dist = haversine_distance(
                        cluster[0]['lat'], cluster[0]['lng'],
                        pin['lat'], pin['lng']
                    )
                    
                    if dist < MERGE_DISTANCE_KM:
                        cluster.append(pin)
                        added_to_cluster = True
                        break
                
                if not added_to_cluster:
                    clusters.append([pin])
            
            # ê° í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ì˜ ì—­ìœ¼ë¡œ ë³‘í•©
            for cluster in clusters:
                if len(cluster) == 1:
                    pin = cluster[0]
                    pin['title'] = name
                    merged_pins.append(pin)
                else:
                    # í™˜ìŠ¹ì—­ - ë…¸ì„  ì •ë³´ í•©ì¹˜ê¸°
                    all_lines = []
                    for pin in cluster:
                        if pin['description']:
                            for line in pin['description'].split(', '):
                                if line and line not in all_lines:
                                    all_lines.append(line)
                    
                    # ì²« ë²ˆì§¸ ì—­ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                    merged = cluster[0].copy()
                    merged['title'] = name
                    merged['description'] = ', '.join(all_lines)
                    merged_pins.append(merged)
    
    return merged_pins


def convert_to_pins(docs):
    """ì¹´ì¹´ì˜¤ API ê²°ê³¼ë¥¼ í•€ ë°ì´í„°ë¡œ ë³€í™˜"""
    pins = []
    
    for doc in docs:
        name = doc.get('place_name', '')
        category = doc.get('category_name', '')
        lines = extract_lines(name, category)
        
        description = ', '.join(lines) if lines else ''
        
        pins.append({
            "title": name,
            "lat": float(doc.get('y')),
            "lng": float(doc.get('x')),
            "address": doc.get('road_address_name') or doc.get('address_name'),
            "description": description,
            "url": doc.get('place_url'),
            "region": get_region(doc)
        })
    
    return pins


def main():
    print(f"ğŸš‡ {NAME} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # 1. Fetch
    raw = fetch_stations()
    print(f"ğŸ“¥ ê²€ìƒ‰ ê²°ê³¼: {len(raw)}ê°œ")
    
    # 2. Save raw
    raw_path = os.path.join(os.path.dirname(__file__), f'{NAME}_raw.json')
    with open(raw_path, 'w', encoding='utf-8') as f:
        json.dump(raw, f, ensure_ascii=False, indent=2)
    
    # 3. Filter
    filtered = [doc for doc in raw if filter_station(doc)]
    print(f"ğŸ” í•„í„°ë§ í›„: {len(filtered)}ê°œ")
    
    # 4. Convert to pins
    pins = convert_to_pins(filtered)
    
    # 5. Remove duplicates by title + address
    seen = set()
    unique_pins = []
    for pin in pins:
        key = (pin['title'], pin['address'])
        if key not in seen:
            seen.add(key)
            unique_pins.append(pin)
    
    print(f"âœ¨ ì¤‘ë³µ ì œê±° í›„: {len(unique_pins)}ê°œ")
    
    # 6. Merge transfer stations (within 500m)
    merged_pins = merge_transfer_stations(unique_pins)
    print(f"ğŸ”„ í™˜ìŠ¹ì—­ ë³‘í•© í›„: {len(merged_pins)}ê°œ")
    
    # 7. Save
    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', f'{LIST_ID}.json')
    with open(data_path, 'w', encoding='utf-8') as f:
        json.dump({"pins": merged_pins}, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {data_path} ì €ì¥ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

